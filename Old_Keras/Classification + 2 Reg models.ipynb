{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boring-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# IMPORT LIBS\n",
    "#####################\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io, transform\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import albumentations as A\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "#####################\n",
    "# SET CONSTANTS\n",
    "#####################\n",
    "\n",
    "INPUT_PATH = Path('../input')\n",
    "OUTPUT_PATH = Path('../output')\n",
    "TRAIN_PATH = INPUT_PATH / 'idao_dataset' / 'train'\n",
    "PRIVATE_PATH = INPUT_PATH / 'idao_dataset' / 'private_test'\n",
    "PUBLIC_PATH = INPUT_PATH / 'idao_dataset' / 'public_test'\n",
    "\n",
    "RANDOM_SEED = 4444\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BATCH_SIZE = 32\n",
    "    TRAINING_EPOCHS = 60\n",
    "    VALIDATION_STEPS_PER_EPOCH = 5\n",
    "    VALIDATION_EPOCHS = 10\n",
    "    STEPS_PER_EPOCH = 30\n",
    "    EARLY_STOP_PATIENCE = 5\n",
    "    \n",
    "    \n",
    "    # Declare an augmentation pipeline\n",
    "    train_transform = A.Compose([\n",
    "        #A.HorizontalFlip(p=0.5),\n",
    "        A.Cutout(num_holes=4, max_h_size=8, max_w_size=8, p=0.3),\n",
    "        A.OneOf([A.RandomContrast(),\n",
    "             A.RandomGamma(),\n",
    "             A.RandomBrightness()],p=0.2),\n",
    "        A.OneOf([A.Blur(p = 0.3),\n",
    "             A.GaussNoise(p=0.3)\n",
    "                ],p=0.5),\n",
    "        A.CLAHE(clip_limit=4, tile_grid_size=(8,8), always_apply=False, p=0.3),\n",
    "        A.Normalize(p=1)\n",
    "    ],)\n",
    "    \n",
    "    validation_transform = A.Compose([\n",
    "        A.Normalize(p=1)\n",
    "    ],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "played-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(img_path):\n",
    "    if len(img_path.split('_')) == 18:\n",
    "        particle_class = 0 # ER\n",
    "        particle_energy = int(img_path.split('_')[7])\n",
    "    else:\n",
    "        particle_class = 1 # HE\n",
    "        particle_energy = int(img_path.split('_')[8])\n",
    "    return [img_path, particle_class, particle_energy]\n",
    "\n",
    "images = glob.glob(str(TRAIN_PATH / '**/*.png'), recursive=True)\n",
    "images = pd.DataFrame(map(getFeatures, images))\n",
    "images.columns = ['path', 'class', 'energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protecting-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# EXTRACT TEST\n",
    "#################\n",
    "\n",
    "# HE - 0, ER - 1\n",
    "\n",
    "he_test_idx = list(images[(images['class'] == 0) & (images['energy'].apply(lambda x: x in [1, 6, 20]))].index)\n",
    "er_test_idx = list(images[(images['class'] == 1) & (images['energy'].apply(lambda x: x in [3, 10, 30]))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "configured-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = he_test_idx + er_test_idx\n",
    "test_images = images.iloc[test_idx]\n",
    "images = images.drop(index = test_idx)\n",
    "\n",
    "train_images, valid_images = train_test_split(images, shuffle = True, random_state = RANDOM_SEED)\n",
    "train_images = train_images.reset_index(drop = True)\n",
    "valid_images = valid_images.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hairy-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(y_binary_true, y_binary_pred, y_reg_true, y_reg_pred):\n",
    "    '''\n",
    "    Competition metric\n",
    "    '''\n",
    "    \n",
    "    roc = roc_auc_score(y_binary_true, y_binary_pred)\n",
    "    mae = mean_absolute_error(y_reg_true, y_reg_pred)\n",
    "    return 1000 * (roc - mae), roc, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "utility-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, transform=None, batch_size=32,  shuffle=True, is_classification = True, LABEL_MAPPER = {1:1,3:3,6:6,10:10,20:20,30:30}):\n",
    "        self.images = images\n",
    "        self.indices = np.arange(len(images))\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.is_classification = is_classification\n",
    "        self.LABEL_MAPPER = LABEL_MAPPER\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "    \n",
    "        X, y = self.__get_data(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X = np.zeros((self.batch_size, 150, 150, 3))\n",
    "        y_class = np.zeros((self.batch_size,))\n",
    "        y_energy = np.zeros((self.batch_size,3))\n",
    "        for i, idx in enumerate(batch):\n",
    "            image=cv2.imread(self.images.iloc[idx, 0])[225:375, 225:375, :]\n",
    "            X[i,] = image\n",
    "            particle_class = self.images.iloc[idx, 1]\n",
    "            particle_energy = self.images.iloc[idx, 2]\n",
    "            y_class[i] = particle_class\n",
    "            y_energy[i] = self.LABEL_MAPPER[particle_energy]\n",
    "        if self.is_classification:\n",
    "            return X / 255.0, y_class\n",
    "\n",
    "        return X / 255.0, y_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naval-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGenerator(train_images, batch_size = Config.BATCH_SIZE, is_classification = True)\n",
    "valid_datagen = DataGenerator(valid_images, batch_size = Config.BATCH_SIZE, is_classification = True)\n",
    "test_datagen = DataGenerator(test_images, batch_size = 1, is_classification = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worth-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data extract\n",
    "\n",
    "X, y_class = train_datagen.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-cannon",
   "metadata": {},
   "source": [
    "## Class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "photographic-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "mobilenet_model = MobileNetV2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "digital-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers as L\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "def create_classification_model(base_model):\n",
    "    x = base_model.output\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = L.Dense(128, activation='relu')(x)\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    predictions = L.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics = ['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fallen-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classification_model(mobilenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "interesting-product",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 16s 441ms/step - loss: 1.0363 - auc: 0.5536 - val_loss: 0.6059 - val_auc: 0.7453\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 12s 391ms/step - loss: 0.5890 - auc: 0.7510 - val_loss: 0.4952 - val_auc: 0.8801\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 11s 378ms/step - loss: 0.5159 - auc: 0.8182 - val_loss: 0.4382 - val_auc: 0.9054\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 10s 325ms/step - loss: 0.4852 - auc: 0.8442 - val_loss: 0.4036 - val_auc: 0.9146\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 10s 340ms/step - loss: 0.4454 - auc: 0.8715 - val_loss: 0.4065 - val_auc: 0.9413\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 10s 351ms/step - loss: 0.4471 - auc: 0.8658 - val_loss: 0.3671 - val_auc: 0.9587\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 10s 330ms/step - loss: 0.4134 - auc: 0.8913 - val_loss: 0.4423 - val_auc: 0.9435\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 9s 315ms/step - loss: 0.3426 - auc: 0.9343 - val_loss: 0.5039 - val_auc: 0.9682\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 10s 325ms/step - loss: 0.4146 - auc: 0.8956 - val_loss: 0.3239 - val_auc: 0.9539\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 9s 315ms/step - loss: 0.3127 - auc: 0.9430 - val_loss: 0.2673 - val_auc: 0.9689\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 10s 324ms/step - loss: 0.3466 - auc: 0.9253 - val_loss: 0.2885 - val_auc: 0.9606\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 9s 312ms/step - loss: 0.3324 - auc: 0.9318 - val_loss: 0.2648 - val_auc: 0.9593\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 9s 301ms/step - loss: 0.3132 - auc: 0.9366 - val_loss: 0.2849 - val_auc: 0.9596\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 9s 303ms/step - loss: 0.2743 - auc: 0.9522 - val_loss: 0.5150 - val_auc: 0.9603\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 9s 290ms/step - loss: 0.3618 - auc: 0.9208 - val_loss: 0.3342 - val_auc: 0.9628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b7032e910>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_datagen, \n",
    "    steps_per_epoch = Config.STEPS_PER_EPOCH, \n",
    "    validation_data = valid_datagen, \n",
    "    validation_steps = Config.VALIDATION_STEPS_PER_EPOCH, \n",
    "    epochs = Config.TRAINING_EPOCHS,\n",
    "    verbose = 1,\n",
    "    callbacks = [earlystop]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "violent-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 19s 375ms/step - loss: 0.3105 - auc: 0.9682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3105323016643524, 0.968234658241272]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_datagen, batch_size=64, steps=valid_images.shape[0] // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "pediatric-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 11ms/step - loss: 0.9348 - auc: 0.7222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9347938895225525, 0.7222222089767456]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_datagen, batch_size=1, steps=test_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "national-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(OUTPUT_PATH / 'models' / 'cnn_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-story",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upset-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "mobilenet_model = MobileNetV2(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conceptual-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENERGY2CLASS_HE = {\n",
    "    1: [1, 0, 0],\n",
    "    6: [0, 1, 0],\n",
    "    20: [0, 0, 1],\n",
    "    \n",
    "}\n",
    "\n",
    "ENERGY2CLASS_ER = {\n",
    "    3: [1, 0, 0],\n",
    "    10: [0, 1, 0],\n",
    "    30: [0, 0, 1],\n",
    "    \n",
    "}\n",
    "\n",
    "ENERGY2CLASS_HE_TEST = {\n",
    "    3: [1, 0, 0],\n",
    "    10: [0, 1, 0],\n",
    "    30: [0, 0, 1],\n",
    "    \n",
    "}\n",
    "\n",
    "ENERGY2CLASS_ER_TEST = {\n",
    "    1: [1, 0, 0],\n",
    "    6: [0, 1, 0],\n",
    "    20: [0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "overall-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS2ENERGY_HE = {\n",
    "    0:1,\n",
    "    1:6,\n",
    "    2:20\n",
    "}\n",
    "\n",
    "CLASS2ENERGY_ER = {\n",
    "    0:3,\n",
    "    1:10,\n",
    "    2:30,\n",
    "    \n",
    "}\n",
    "\n",
    "CLASS2ENERGY_HE_TEST = {\n",
    "    0:3,\n",
    "    1:10,\n",
    "    2:30,\n",
    "    \n",
    "}\n",
    "\n",
    "CLASS2ENERGY_ER_TEST = {\n",
    "    0:1,\n",
    "    1:6,\n",
    "    2:20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-baltimore",
   "metadata": {},
   "source": [
    "### ER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "strong-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers as L\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "def create_classification_model(base_model):\n",
    "    x = base_model.output\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    # let's add a fully-connected layer\n",
    "    x = L.Dense(512, activation='relu')(x)\n",
    "    x = L.Dense(64, activation='relu')(x)\n",
    "    # and a logistic layer -- let's say we have 200 classes\n",
    "    predictions = L.Dense(3, activation='sigmoid')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "downtown-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_classification_model(mobilenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "danish-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGenerator(train_images[train_images['class'] == 0].reset_index(drop=True), batch_size = Config.BATCH_SIZE, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_ER)\n",
    "valid_datagen = DataGenerator(valid_images[valid_images['class'] == 0].reset_index(drop=True), batch_size = Config.BATCH_SIZE, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_ER)\n",
    "test_datagen = DataGenerator(test_images[test_images['class']==0].reset_index(drop=True), batch_size = 1, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_ER_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dietary-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 11s 267ms/step - loss: 1.5608 - categorical_accuracy: 0.4583 - val_loss: 0.6732 - val_categorical_accuracy: 0.6938\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.3916 - categorical_accuracy: 0.8548 - val_loss: 0.1777 - val_categorical_accuracy: 0.9688\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 7s 228ms/step - loss: 0.1569 - categorical_accuracy: 0.9616 - val_loss: 0.1105 - val_categorical_accuracy: 0.9563\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.1335 - categorical_accuracy: 0.9582 - val_loss: 0.1851 - val_categorical_accuracy: 0.9563\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1781 - categorical_accuracy: 0.9483 - val_loss: 0.0618 - val_categorical_accuracy: 0.9875\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 7s 229ms/step - loss: 0.0807 - categorical_accuracy: 0.9654 - val_loss: 0.0960 - val_categorical_accuracy: 0.9875\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 7s 229ms/step - loss: 0.1476 - categorical_accuracy: 0.9367 - val_loss: 0.1244 - val_categorical_accuracy: 0.9500\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.0663 - categorical_accuracy: 0.9765 - val_loss: 0.3930 - val_categorical_accuracy: 0.8375\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1758 - categorical_accuracy: 0.9401 - val_loss: 0.0948 - val_categorical_accuracy: 0.9688\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.0667 - categorical_accuracy: 0.9828 - val_loss: 0.0251 - val_categorical_accuracy: 0.9937\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.0962 - categorical_accuracy: 0.9699 - val_loss: 0.0255 - val_categorical_accuracy: 0.9812\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.0444 - categorical_accuracy: 0.9822 - val_loss: 0.0183 - val_categorical_accuracy: 0.9937\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 7s 222ms/step - loss: 0.0797 - categorical_accuracy: 0.9768 - val_loss: 0.0105 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 7s 222ms/step - loss: 0.0150 - categorical_accuracy: 0.9941 - val_loss: 0.0017 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 7s 222ms/step - loss: 0.1661 - categorical_accuracy: 0.9758 - val_loss: 0.0060 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.0143 - categorical_accuracy: 0.9952 - val_loss: 0.0688 - val_categorical_accuracy: 0.9812\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.0909 - categorical_accuracy: 0.9641 - val_loss: 0.0235 - val_categorical_accuracy: 0.9937\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.0751 - categorical_accuracy: 0.9797 - val_loss: 0.0032 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.0095 - categorical_accuracy: 0.9981 - val_loss: 0.0438 - val_categorical_accuracy: 0.9875\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.0524 - categorical_accuracy: 0.9892 - val_loss: 0.0031 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.0236 - categorical_accuracy: 0.9944 - val_loss: 0.0052 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.0243 - categorical_accuracy: 0.9946 - val_loss: 0.0276 - val_categorical_accuracy: 0.9875\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.0519 - categorical_accuracy: 0.9857 - val_loss: 0.0311 - val_categorical_accuracy: 0.9937\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 7s 222ms/step - loss: 0.0434 - categorical_accuracy: 0.9894 - val_loss: 0.0179 - val_categorical_accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1dc43b5dd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_datagen, \n",
    "    steps_per_epoch = Config.STEPS_PER_EPOCH, \n",
    "    validation_data = valid_datagen, \n",
    "    validation_steps = Config.VALIDATION_STEPS_PER_EPOCH, \n",
    "    epochs = Config.TRAINING_EPOCHS,\n",
    "    verbose = 1,\n",
    "    callbacks = [earlystop]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nervous-donna",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 10s 184ms/step - loss: 0.0324 - categorical_accuracy: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03242973983287811, 0.9897836446762085]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_datagen, batch_size=64, steps=valid_images.shape[0] // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dried-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1689/1689 [==============================] - 13s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = DataGenerator(valid_images[valid_images['class'] == 0], batch_size = 1, is_classification = False, shuffle = False)\n",
    "validation_preds = model.predict_generator(valid_datagen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "emotional-winner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16814683244523387"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "Y_pred = np.vectorize(CLASS2ENERGY_ER.get)(np.argmax(validation_preds, axis =1 ))\n",
    "Y_true = valid_images[valid_images['class'] == 0]['energy']\n",
    "mean_absolute_error(Y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "minus-sword",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5943 - categorical_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5942790508270264, 0.8333333134651184]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_datagen, batch_size=1, steps=test_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brilliant-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "test_datagen = DataGenerator(test_images[test_images['class'] == 0], batch_size = 1, is_classification = False, shuffle = False)\n",
    "test_preds = model.predict_generator(test_datagen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "traditional-pledge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = np.vectorize(CLASS2ENERGY_ER_TEST.get)(np.argmax(test_preds, axis =1 ))\n",
    "Y_true = test_images[test_images['class'] == 0]['energy']\n",
    "mean_absolute_error(Y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aggregate-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(OUTPUT_PATH / 'models' / 'cnn_energy_er')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-curtis",
   "metadata": {},
   "source": [
    "### HE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "noticed-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "model = create_classification_model(mobilenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "simple-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGenerator(train_images[train_images['class'] == 1].reset_index(drop=True), batch_size = Config.BATCH_SIZE, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_HE)\n",
    "valid_datagen = DataGenerator(valid_images[valid_images['class'] == 1].reset_index(drop=True), batch_size = Config.BATCH_SIZE, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_HE)\n",
    "test_datagen = DataGenerator(test_images[test_images['class']==1].reset_index(drop=True), batch_size = 1, is_classification = False, LABEL_MAPPER=ENERGY2CLASS_HE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accessory-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "30/30 [==============================] - 11s 264ms/step - loss: 1.9996 - categorical_accuracy: 0.5212 - val_loss: 1.0164 - val_categorical_accuracy: 0.6562\n",
      "Epoch 2/60\n",
      "30/30 [==============================] - 7s 228ms/step - loss: 0.5544 - categorical_accuracy: 0.7378 - val_loss: 0.4786 - val_categorical_accuracy: 0.6812\n",
      "Epoch 3/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.4735 - categorical_accuracy: 0.7117 - val_loss: 0.4521 - val_categorical_accuracy: 0.6750\n",
      "Epoch 4/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.4447 - categorical_accuracy: 0.7429 - val_loss: 0.3386 - val_categorical_accuracy: 0.8000\n",
      "Epoch 5/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.3453 - categorical_accuracy: 0.8164 - val_loss: 0.2875 - val_categorical_accuracy: 0.8625\n",
      "Epoch 6/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.2999 - categorical_accuracy: 0.8507 - val_loss: 0.2207 - val_categorical_accuracy: 0.9438\n",
      "Epoch 7/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.2869 - categorical_accuracy: 0.8813 - val_loss: 0.2701 - val_categorical_accuracy: 0.8750\n",
      "Epoch 8/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.2032 - categorical_accuracy: 0.9236 - val_loss: 0.1580 - val_categorical_accuracy: 0.9688\n",
      "Epoch 9/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.2505 - categorical_accuracy: 0.9015 - val_loss: 0.1346 - val_categorical_accuracy: 0.9750\n",
      "Epoch 10/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.2481 - categorical_accuracy: 0.8767 - val_loss: 0.1662 - val_categorical_accuracy: 0.9312\n",
      "Epoch 11/60\n",
      "30/30 [==============================] - 7s 230ms/step - loss: 0.2762 - categorical_accuracy: 0.8775 - val_loss: 0.1368 - val_categorical_accuracy: 0.9438\n",
      "Epoch 12/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.2204 - categorical_accuracy: 0.9007 - val_loss: 0.2055 - val_categorical_accuracy: 0.9000\n",
      "Epoch 13/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1963 - categorical_accuracy: 0.9104 - val_loss: 0.1593 - val_categorical_accuracy: 0.9312\n",
      "Epoch 14/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.2008 - categorical_accuracy: 0.9194 - val_loss: 0.1290 - val_categorical_accuracy: 0.9438\n",
      "Epoch 15/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.1893 - categorical_accuracy: 0.9120 - val_loss: 0.0776 - val_categorical_accuracy: 0.9812\n",
      "Epoch 16/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1366 - categorical_accuracy: 0.9503 - val_loss: 0.1120 - val_categorical_accuracy: 0.9688\n",
      "Epoch 17/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.1515 - categorical_accuracy: 0.9397 - val_loss: 0.2623 - val_categorical_accuracy: 0.8938\n",
      "Epoch 18/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1658 - categorical_accuracy: 0.9399 - val_loss: 0.1105 - val_categorical_accuracy: 0.9500\n",
      "Epoch 19/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1755 - categorical_accuracy: 0.9127 - val_loss: 0.1090 - val_categorical_accuracy: 0.9438\n",
      "Epoch 20/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.1469 - categorical_accuracy: 0.9417 - val_loss: 0.3122 - val_categorical_accuracy: 0.8250\n",
      "Epoch 21/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.1345 - categorical_accuracy: 0.9616 - val_loss: 0.1514 - val_categorical_accuracy: 0.9312\n",
      "Epoch 22/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.2161 - categorical_accuracy: 0.9322 - val_loss: 0.0533 - val_categorical_accuracy: 0.9812\n",
      "Epoch 23/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.1992 - categorical_accuracy: 0.9289 - val_loss: 0.0786 - val_categorical_accuracy: 0.9937\n",
      "Epoch 24/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.0974 - categorical_accuracy: 0.9672 - val_loss: 0.2926 - val_categorical_accuracy: 0.8750\n",
      "Epoch 25/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.0903 - categorical_accuracy: 0.9653 - val_loss: 0.0862 - val_categorical_accuracy: 0.9688\n",
      "Epoch 26/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.1011 - categorical_accuracy: 0.9632 - val_loss: 0.0631 - val_categorical_accuracy: 0.9875\n",
      "Epoch 27/60\n",
      "30/30 [==============================] - 7s 221ms/step - loss: 0.1287 - categorical_accuracy: 0.9523 - val_loss: 0.1136 - val_categorical_accuracy: 0.9625\n",
      "Epoch 28/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.1038 - categorical_accuracy: 0.9540 - val_loss: 0.0927 - val_categorical_accuracy: 0.9812\n",
      "Epoch 29/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.1188 - categorical_accuracy: 0.9577 - val_loss: 0.0593 - val_categorical_accuracy: 0.9812\n",
      "Epoch 30/60\n",
      "30/30 [==============================] - 7s 221ms/step - loss: 0.1097 - categorical_accuracy: 0.9595 - val_loss: 0.1193 - val_categorical_accuracy: 0.9625\n",
      "Epoch 31/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.0887 - categorical_accuracy: 0.9632 - val_loss: 0.0520 - val_categorical_accuracy: 0.9812\n",
      "Epoch 32/60\n",
      "30/30 [==============================] - 7s 223ms/step - loss: 0.1337 - categorical_accuracy: 0.9478 - val_loss: 0.0450 - val_categorical_accuracy: 0.9875\n",
      "Epoch 33/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.1209 - categorical_accuracy: 0.9593 - val_loss: 0.2634 - val_categorical_accuracy: 0.8813\n",
      "Epoch 34/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.1251 - categorical_accuracy: 0.9608 - val_loss: 0.0313 - val_categorical_accuracy: 0.9937\n",
      "Epoch 35/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.0831 - categorical_accuracy: 0.9768 - val_loss: 0.4543 - val_categorical_accuracy: 0.8125\n",
      "Epoch 36/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.1075 - categorical_accuracy: 0.9541 - val_loss: 0.0365 - val_categorical_accuracy: 0.9812\n",
      "Epoch 37/60\n",
      "30/30 [==============================] - 7s 227ms/step - loss: 0.0870 - categorical_accuracy: 0.9709 - val_loss: 0.0687 - val_categorical_accuracy: 0.9688\n",
      "Epoch 38/60\n",
      "30/30 [==============================] - 7s 229ms/step - loss: 0.1180 - categorical_accuracy: 0.9549 - val_loss: 0.1029 - val_categorical_accuracy: 0.9500\n",
      "Epoch 39/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.0945 - categorical_accuracy: 0.9601 - val_loss: 0.0399 - val_categorical_accuracy: 0.9812\n",
      "Epoch 40/60\n",
      "30/30 [==============================] - 7s 222ms/step - loss: 0.0790 - categorical_accuracy: 0.9791 - val_loss: 0.2077 - val_categorical_accuracy: 0.9250\n",
      "Epoch 41/60\n",
      "30/30 [==============================] - 7s 226ms/step - loss: 0.0936 - categorical_accuracy: 0.9684 - val_loss: 0.0779 - val_categorical_accuracy: 0.9750\n",
      "Epoch 42/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.0660 - categorical_accuracy: 0.9827 - val_loss: 0.0908 - val_categorical_accuracy: 0.9563\n",
      "Epoch 43/60\n",
      "30/30 [==============================] - 7s 224ms/step - loss: 0.0743 - categorical_accuracy: 0.9760 - val_loss: 0.0467 - val_categorical_accuracy: 0.9812\n",
      "Epoch 44/60\n",
      "30/30 [==============================] - 7s 225ms/step - loss: 0.0559 - categorical_accuracy: 0.9844 - val_loss: 0.0382 - val_categorical_accuracy: 0.9937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f19c6b74910>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_datagen, \n",
    "    steps_per_epoch = Config.STEPS_PER_EPOCH, \n",
    "    validation_data = valid_datagen, \n",
    "    validation_steps = Config.VALIDATION_STEPS_PER_EPOCH, \n",
    "    epochs = Config.TRAINING_EPOCHS,\n",
    "    verbose = 1,\n",
    "    callbacks = [earlystop]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "charitable-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 9s 179ms/step - loss: 0.0593 - categorical_accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05928764119744301, 0.9773284196853638]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_datagen, batch_size=64, steps=valid_images.shape[0] // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baking-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659/1659 [==============================] - 13s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = DataGenerator(valid_images[valid_images['class'] == 1], batch_size = 1, is_classification = False, shuffle = False)\n",
    "validation_preds = model.predict_generator(valid_datagen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "surprising-motivation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11151295961422544"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "Y_pred = np.vectorize(CLASS2ENERGY_HE.get)(np.argmax(validation_preds, axis =1 ))\n",
    "Y_true = valid_images[valid_images['class'] == 1]['energy']\n",
    "mean_absolute_error(Y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "composed-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3774 - categorical_accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37743833661079407, 0.8333333134651184]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_datagen, batch_size=1, steps=test_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "criminal-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "test_datagen = DataGenerator(test_images[test_images['class'] == 1], batch_size = 1, is_classification = False, shuffle = False)\n",
    "test_preds = model.predict_generator(test_datagen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "reverse-virginia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1666666666666667"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = np.vectorize(CLASS2ENERGY_HE_TEST.get)(np.argmax(test_preds, axis =1 ))\n",
    "Y_true = test_images[test_images['class'] == 1]['energy']\n",
    "mean_absolute_error(Y_pred, Y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "returning-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(OUTPUT_PATH / 'models' / 'cnn_energy_he')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-mainland",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "therapeutic-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, transform=None, batch_size=32,  shuffle=True):\n",
    "        self.images = images\n",
    "        self.indices = np.arange(len(images))\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.indices[k] for k in index]\n",
    "    \n",
    "        X = self.__get_data(batch)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X = np.zeros((self.batch_size, 150, 150, 3))\n",
    "        for i, idx in enumerate(batch):\n",
    "            image=cv2.imread(str(self.images.iloc[idx, 3]))[225:375, 225:375, :]\n",
    "            X[i,] = image\n",
    "\n",
    "        return X / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "norwegian-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "private_test = glob.glob(str(PRIVATE_PATH / '**/*.png'), recursive=True)\n",
    "public_test = glob.glob(str(PUBLIC_PATH / '**/*.png'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "comparative-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(INPUT_PATH / 'track1_predictions_example.csv')\n",
    "private_ids = [t.split('/')[-1].split('.')[0] for t in private_test]\n",
    "public_ids = [t.split('/')[-1].split('.')[0] for t in public_test]\n",
    "\n",
    "sample_submission['path'] = sample_submission['id'].apply(lambda x: PRIVATE_PATH / f'{x}.png' if x in private_ids else PUBLIC_PATH / f'{x}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "humanitarian-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_datagen = PredictDataGenerator(sample_submission, shuffle = False, batch_size = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "varied-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 122s 301ms/step\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# PREDICT WITH CLASSIFICATION MODEL\n",
    "####################################\n",
    "\n",
    "cnn_classification = keras.models.load_model(OUTPUT_PATH / 'models' / 'cnn_classification')\n",
    "y_pred_class = cnn_classification.predict_generator(prediction_datagen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_regression_er = keras.models.load_model(OUTPUT_PATH / 'models' / 'cnn_energy_er')\n",
    "cnn_regression_he = keras.models.load_model(OUTPUT_PATH / 'models' / 'cnn_energy_he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "caring-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/549 [==============================] - 5s 7ms/step\n",
      "953/953 [==============================] - 7s 7ms/step\n",
      "5338/5338 [==============================] - 37s 7ms/step\n",
      "9724/9724 [==============================] - 69s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# PREDICT WITH REGRESSION MODEL\n",
    "####################################\n",
    "\n",
    "\n",
    "CONDITION_PUBLIC_ER = (y_pred_class.reshape(-1) < 0.5) & (sample_submission['path'].apply(lambda x: 'public_test' in str(x)))\n",
    "CONDITION_PUBLIC_HE = (y_pred_class.reshape(-1) >= 0.5) & (sample_submission['path'].apply(lambda x: 'public_test' in str(x)))\n",
    "\n",
    "CONDITION_PRIVATE_ER = (y_pred_class.reshape(-1) < 0.5) & (sample_submission['path'].apply(lambda x: 'private_test' in str(x)))\n",
    "CONDITION_PRIVATE_HE = (y_pred_class.reshape(-1) >= 0.5) & (sample_submission['path'].apply(lambda x: 'private_test' in str(x)))\n",
    "\n",
    "public_er_datagen = PredictDataGenerator(sample_submission[CONDITION_PUBLIC_ER], shuffle = False, batch_size = 1)\n",
    "public_he_datagen = PredictDataGenerator(sample_submission[CONDITION_PUBLIC_HE], shuffle = False, batch_size = 1)\n",
    "private_er_datagen = PredictDataGenerator(sample_submission[CONDITION_PRIVATE_ER], shuffle = False, batch_size = 1)\n",
    "private_he_datagen = PredictDataGenerator(sample_submission[CONDITION_PRIVATE_HE], shuffle = False, batch_size = 1)\n",
    "\n",
    "y_pred_energy = cnn_regression_er.predict_generator(public_er_datagen, verbose = 1)\n",
    "sample_submission.iloc[CONDITION_PUBLIC_ER, 2] = np.vectorize(CLASS2ENERGY_ER.get)(np.argmax(y_pred_energy, axis = 1))\n",
    "\n",
    "y_pred_energy = cnn_regression_he.predict_generator(public_he_datagen, verbose = 1)\n",
    "sample_submission.iloc[CONDITION_PUBLIC_HE, 2] = np.vectorize(CLASS2ENERGY_HE.get)(np.argmax(y_pred_energy, axis = 1))\n",
    "\n",
    "y_pred_energy = cnn_regression_er.predict_generator(private_er_datagen, verbose = 1)\n",
    "sample_submission.iloc[CONDITION_PRIVATE_ER, 2] = np.vectorize(CLASS2ENERGY_ER_TEST.get)(np.argmax(y_pred_energy, axis = 1))\n",
    "\n",
    "y_pred_energy = cnn_regression_he.predict_generator(private_he_datagen, verbose = 1)\n",
    "sample_submission.iloc[CONDITION_PRIVATE_HE, 2] = np.vectorize(CLASS2ENERGY_HE_TEST.get)(np.argmax(y_pred_energy, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dynamic-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['classification_predictions'] = y_pred_class.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "quantitative-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.drop(columns = ['path']).to_csv(OUTPUT_PATH / 'predictions' / 'prediction.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
