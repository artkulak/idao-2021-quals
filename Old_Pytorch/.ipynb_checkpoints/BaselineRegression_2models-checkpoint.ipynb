{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# IMPORT LIBS\n",
    "#####################\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models, utils\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "from skimage import io, transform\n",
    "from torch.optim import lr_scheduler\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "#####################\n",
    "# SET CONSTANTS\n",
    "#####################\n",
    "\n",
    "INPUT_PATH = Path('../input')\n",
    "OUTPUT_PATH = Path('../output')\n",
    "TRAIN_PATH = INPUT_PATH / 'idao_dataset' / 'train'\n",
    "PRIVATE_PATH = INPUT_PATH / 'idao_dataset' / 'private_test'\n",
    "PUBLIC_PATH = INPUT_PATH / 'idao_dataset' / 'public_test'\n",
    "\n",
    "RANDOM_SEED = 4444\n",
    "\n",
    "MAPPER = {\n",
    "    1:0,\n",
    "    3:1,\n",
    "    6:2,\n",
    "    10:3,\n",
    "    20:4,\n",
    "    30:5,\n",
    "}\n",
    "\n",
    "REVERSE_MAPPER = {\n",
    "    0: 1,\n",
    "    1: 3,\n",
    "    2: 6,\n",
    "    3: 10,\n",
    "    4: 20,\n",
    "    5: 30\n",
    "}\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(img_path):\n",
    "    if len(img_path.split('_')) == 18:\n",
    "        particle_class = 0 # ER\n",
    "        particle_energy = int(img_path.split('_')[7])\n",
    "    else:\n",
    "        particle_class = 1 # HE\n",
    "        particle_energy = int(img_path.split('_')[8])\n",
    "    return [img_path, particle_class, particle_energy]\n",
    "\n",
    "images = glob.glob(str(TRAIN_PATH / '**/*.png'), recursive=True)\n",
    "images = pd.DataFrame(map(getFeatures, images))\n",
    "images.columns = ['path', 'class', 'energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = train_test_split(images, shuffle = True, random_state = RANDOM_SEED)\n",
    "train_images = train_images.reset_index(drop = True)\n",
    "test_images = test_images.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(y_binary_true, y_binary_pred, y_reg_true, y_reg_pred):\n",
    "    '''\n",
    "    Competition metric\n",
    "    '''\n",
    "    \n",
    "    roc = roc_auc_score(y_binary_true, y_binary_pred)\n",
    "    mae = mean_absolute_error(y_reg_true, y_reg_pred)\n",
    "    return 1000 * (roc - mae), roc, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter(Dataset):\n",
    "    def __init__(self, images, train=True, transform=None):\n",
    " \n",
    "        self.images = images\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=cv2.imread(self.images.iloc[idx, 0])[225:375, 225:375, :]\n",
    "        particle_class = self.images.iloc[idx, 1]\n",
    "        particle_energy = self.images.iloc[idx, 2]\n",
    "        \n",
    "        sample={\n",
    "            'image': np.uint8(image), \n",
    "            'particle_class': particle_class,\n",
    "            'particle_energy': particle_energy\n",
    "            }\n",
    "\n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample['image']=self.transform(sample['image'])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = transforms.Compose([\n",
    "    #transforms.ToPILImage(),\n",
    "    #transforms.Resize([256,256]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = DataGetter(train_images, train=True, transform=augs)\n",
    "transformed_test_data = DataGetter(test_images, train=False, transform=augs)\n",
    "\n",
    "train_dataloader = DataLoader(transformed_train_data, batch_size=32, shuffle=True) #, num_workers=2\n",
    "test_dataloader = DataLoader(transformed_test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CNNClassification(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(CNNClassification, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.model = models.resnet18()\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.fc0 = nn.Linear(512, 64)\n",
    "        self.fc1 = nn.Linear(64, 2)  # For classification\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.relu(self.fc0(x))\n",
    "        particle_class = torch.softmax(self.fc1(x), dim = 1)\n",
    "        return {'particle_class': particle_class}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_cl = CNNClassification(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in train_dataloader:\n",
    "#     img = item['image']\n",
    "#     break\n",
    "# model_CNN(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For binary output:particle_class\n",
    "criterion_binary= nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model_cl.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion_binary, optimizer, n_epochs=5):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(0, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        comp_metric = 0 \n",
    "        comp_val_metric = 0\n",
    "        batches = 0\n",
    "        val_batches = 0\n",
    "        max_batches = 512\n",
    "        # train the model #\n",
    "        model.train()\n",
    "        for batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "            if batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_class = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_class'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_class = output['particle_class']\n",
    "    \n",
    "            \n",
    "            particle_class = particle_class.squeeze().type(torch.LongTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_binary = label_class.cpu().detach().numpy()[:, 1]\n",
    "            y_true_binary = particle_class.cpu().detach().numpy()\n",
    "            \n",
    "            roc = roc_auc_score(y_true_binary, y_pred_binary) \n",
    "\n",
    "            loss_binary = criterion_binary(label_class, particle_class)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_binary\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f ROC AUC %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss, roc))\n",
    "            batches += 1\n",
    "        # validate the model #\n",
    "        model.eval()\n",
    "        \n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "            if val_batches > max_batches:\n",
    "                break\n",
    "            image, particle_class = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_class'].to(device)\n",
    "                                              \n",
    "            \n",
    "            output = model(image)\n",
    "\n",
    "            label_class = output['particle_class']\n",
    "\n",
    "            \n",
    "            particle_class = particle_class.squeeze().type(torch.LongTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_binary = label_class.cpu().detach().numpy()[:, 1]\n",
    "            y_true_binary = particle_class.cpu().detach().numpy()\n",
    "            \n",
    "            y_preds.extend(list(y_pred_binary))\n",
    "            y_trues.extend(list(y_true_binary))\n",
    "            \n",
    "            # calculate loss\n",
    "            loss_binary = criterion_binary(label_class, particle_class)\n",
    "            \n",
    "            loss = loss_binary\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            val_batches += 1\n",
    "        \n",
    "        roc_val = roc_auc_score(y_trues, y_preds) \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t ROC AUC {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss, roc_val))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model, OUTPUT_PATH / 'model_classification.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1 loss: 0.699211 ROC AUC 0.351562\n",
      "Epoch 0, Batch 51 loss: 0.630493 ROC AUC 0.991903\n",
      "Epoch 0, Batch 101 loss: 0.503121 ROC AUC 1.000000\n",
      "Epoch 0, Batch 151 loss: 0.444496 ROC AUC 1.000000\n",
      "Epoch 0, Batch 201 loss: 0.416438 ROC AUC 1.000000\n",
      "Epoch 0, Batch 251 loss: 0.399363 ROC AUC 1.000000\n",
      "Epoch 0, Batch 301 loss: 0.388357 ROC AUC 1.000000\n",
      "Epoch: 0 \tTraining Loss: 0.385218 \tValidation Loss: 0.814594 \t ROC AUC 0.915035\n",
      "Validation loss decreased (inf --> 0.814594).  Saving model ...\n",
      "Epoch 1, Batch 1 loss: 0.486280 ROC AUC 0.936508\n",
      "Epoch 1, Batch 51 loss: 0.329172 ROC AUC 1.000000\n",
      "Epoch 1, Batch 101 loss: 0.328057 ROC AUC 1.000000\n",
      "Epoch 1, Batch 151 loss: 0.326105 ROC AUC 1.000000\n",
      "Epoch 1, Batch 201 loss: 0.327151 ROC AUC 1.000000\n",
      "Epoch 1, Batch 251 loss: 0.329231 ROC AUC 1.000000\n",
      "Epoch 1, Batch 301 loss: 0.327522 ROC AUC 1.000000\n",
      "Epoch: 1 \tTraining Loss: 0.327619 \tValidation Loss: 0.368265 \t ROC AUC 0.999654\n",
      "Validation loss decreased (0.814594 --> 0.368265).  Saving model ...\n",
      "Epoch 2, Batch 1 loss: 0.318620 ROC AUC 1.000000\n",
      "Epoch 2, Batch 51 loss: 0.317412 ROC AUC 1.000000\n",
      "Epoch 2, Batch 101 loss: 0.317506 ROC AUC 1.000000\n",
      "Epoch 2, Batch 151 loss: 0.319948 ROC AUC 1.000000\n",
      "Epoch 2, Batch 201 loss: 0.319633 ROC AUC 1.000000\n",
      "Epoch 2, Batch 251 loss: 0.318378 ROC AUC 1.000000\n",
      "Epoch 2, Batch 301 loss: 0.329457 ROC AUC 0.898785\n",
      "Epoch: 2 \tTraining Loss: 0.332279 \tValidation Loss: 0.341347 \t ROC AUC 0.986619\n",
      "Validation loss decreased (0.368265 --> 0.341347).  Saving model ...\n",
      "Epoch 3, Batch 1 loss: 0.340855 ROC AUC 1.000000\n",
      "Epoch 3, Batch 51 loss: 0.315337 ROC AUC 1.000000\n",
      "Epoch 3, Batch 101 loss: 0.319218 ROC AUC 1.000000\n",
      "Epoch 3, Batch 151 loss: 0.322603 ROC AUC 1.000000\n",
      "Epoch 3, Batch 201 loss: 0.320628 ROC AUC 1.000000\n",
      "Epoch 3, Batch 251 loss: 0.320444 ROC AUC 1.000000\n",
      "Epoch 3, Batch 301 loss: 0.320042 ROC AUC 1.000000\n",
      "Epoch: 3 \tTraining Loss: 0.320190 \tValidation Loss: 0.315150 \t ROC AUC 0.999978\n",
      "Validation loss decreased (0.341347 --> 0.315150).  Saving model ...\n",
      "Epoch 4, Batch 1 loss: 0.344513 ROC AUC 0.970238\n",
      "Epoch 4, Batch 51 loss: 0.316481 ROC AUC 1.000000\n",
      "Epoch 4, Batch 101 loss: 0.318562 ROC AUC 1.000000\n",
      "Epoch 4, Batch 151 loss: 0.318085 ROC AUC 1.000000\n",
      "Epoch 4, Batch 201 loss: 0.319711 ROC AUC 1.000000\n",
      "Epoch 4, Batch 251 loss: 0.318641 ROC AUC 1.000000\n",
      "Epoch 4, Batch 301 loss: 0.317852 ROC AUC 1.000000\n",
      "Epoch: 4 \tTraining Loss: 0.317846 \tValidation Loss: 0.314971 \t ROC AUC 0.999999\n",
      "Validation loss decreased (0.315150 --> 0.314971).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model_cl = train_model(model_cl, criterion_binary, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CNNReg(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(CNNReg, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.model = models.resnet18()\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.fc0 = nn.Linear(512, 64)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.relu(self.fc0(x))\n",
    "        particle_energy = self.fc1(x)\n",
    "        return {'particle_energy': particle_energy}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reg = CNNReg(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_reg= nn.L1Loss()\n",
    "optimizer = optim.RMSprop(model_reg.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in train_dataloader:\n",
    "#     img = item['image']\n",
    "#     break\n",
    "# model_reg(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_reg(model, criterion_reg, optimizer, n_epochs=5):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(0, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        comp_metric = 0 \n",
    "        comp_val_metric = 0\n",
    "        batches = 0\n",
    "        val_batches = 0\n",
    "        max_batches = 512\n",
    "        # train the model #\n",
    "        model.train()\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "            if batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_energy = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_energy'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_energy = output['particle_energy']\n",
    "    \n",
    "            \n",
    "            particle_energy = particle_energy.squeeze().type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_reg = label_energy.cpu().detach().numpy()\n",
    "            #y_pred_reg = [reverse_mapping[x] for x in list(np.argmax(y_pred_reg, axis = 1))]\n",
    "            \n",
    "            y_true_reg = particle_energy.cpu().detach().numpy()\n",
    "            #y_true_reg = [reverse_mapping[x] for x in list(y_true_reg)]\n",
    "            \n",
    "            \n",
    "            mae = mean_absolute_error(y_true_reg, y_pred_reg) \n",
    "\n",
    "            loss_reg = criterion_reg(label_energy.reshape(-1), particle_energy)\n",
    "            \n",
    "            y_preds.extend(list(y_pred_reg))\n",
    "            y_trues.extend(list(y_true_reg))\n",
    "            \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_reg\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f MAE %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss, mae))\n",
    "            batches += 1\n",
    "        # validate the model #\n",
    "        model.eval()\n",
    "        \n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "            if val_batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_energy = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_energy'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_energy = output['particle_energy']\n",
    "\n",
    "            \n",
    "            particle_energy = particle_energy.squeeze().type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_reg = label_energy.cpu().detach().numpy()\n",
    "            #y_pred_reg = [reverse_mapping[x] for x in list(np.argmax(y_pred_reg, axis = 1))]\n",
    "            \n",
    "            y_true_reg = particle_energy.cpu().detach().numpy()\n",
    "            #y_true_reg = [reverse_mapping[x] for x in list(y_true_reg)]\n",
    "            \n",
    "            y_preds.extend(list(y_pred_reg))\n",
    "            y_trues.extend(list(y_true_reg))\n",
    "            \n",
    "\n",
    "            loss_reg = criterion_reg(label_energy.reshape(-1), particle_energy)\n",
    "            \n",
    "            loss = loss_reg\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            val_batches += 1\n",
    "        \n",
    "        mae_val = mean_absolute_error(y_trues, y_preds) \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t MAE {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss, mae_val))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model, OUTPUT_PATH / 'model_regression.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1 loss: 13.061502 MAE 13.061502\n",
      "Epoch 0, Batch 51 loss: 3.057503 MAE 1.865019\n",
      "Epoch 0, Batch 101 loss: 2.476733 MAE 2.861814\n",
      "Epoch 0, Batch 151 loss: 2.171927 MAE 1.382367\n",
      "Epoch 0, Batch 201 loss: 2.022554 MAE 3.165423\n",
      "Epoch 0, Batch 251 loss: 1.948667 MAE 0.742521\n",
      "Epoch 0, Batch 301 loss: 1.883224 MAE 4.531523\n",
      "Epoch: 0 \tTraining Loss: 1.862385 \tValidation Loss: 3.914448 \t MAE 3.915007\n",
      "Validation loss decreased (inf --> 3.914448).  Saving model ...\n",
      "Epoch 1, Batch 1 loss: 1.100076 MAE 1.100076\n",
      "Epoch 1, Batch 51 loss: 1.549594 MAE 0.858923\n",
      "Epoch 1, Batch 101 loss: 1.436842 MAE 0.707952\n",
      "Epoch 1, Batch 151 loss: 1.421592 MAE 2.361756\n",
      "Epoch 1, Batch 201 loss: 1.424190 MAE 2.227243\n",
      "Epoch 1, Batch 251 loss: 1.403620 MAE 1.940833\n",
      "Epoch 1, Batch 301 loss: 1.355578 MAE 1.693081\n",
      "Epoch: 1 \tTraining Loss: 1.348728 \tValidation Loss: 2.182770 \t MAE 2.181895\n",
      "Validation loss decreased (3.914448 --> 2.182770).  Saving model ...\n",
      "Epoch 2, Batch 1 loss: 1.382796 MAE 1.382796\n",
      "Epoch 2, Batch 51 loss: 1.320763 MAE 1.488989\n",
      "Epoch 2, Batch 101 loss: 1.213099 MAE 0.814449\n",
      "Epoch 2, Batch 151 loss: 1.260652 MAE 3.011844\n",
      "Epoch 2, Batch 201 loss: 1.236201 MAE 3.241199\n",
      "Epoch 2, Batch 251 loss: 1.213537 MAE 0.648378\n",
      "Epoch 2, Batch 301 loss: 1.199392 MAE 0.918381\n",
      "Epoch: 2 \tTraining Loss: 1.203456 \tValidation Loss: 0.836638 \t MAE 0.836697\n",
      "Validation loss decreased (2.182770 --> 0.836638).  Saving model ...\n",
      "Epoch 3, Batch 1 loss: 0.847180 MAE 0.847180\n",
      "Epoch 3, Batch 51 loss: 0.855304 MAE 0.457480\n",
      "Epoch 3, Batch 101 loss: 0.967880 MAE 1.483543\n",
      "Epoch 3, Batch 151 loss: 0.962494 MAE 0.576389\n",
      "Epoch 3, Batch 201 loss: 0.949959 MAE 1.213289\n",
      "Epoch 3, Batch 251 loss: 0.932548 MAE 1.292479\n",
      "Epoch 3, Batch 301 loss: 0.929495 MAE 0.927102\n",
      "Epoch: 3 \tTraining Loss: 0.923117 \tValidation Loss: 0.924579 \t MAE 0.924814\n",
      "Epoch 4, Batch 1 loss: 0.767799 MAE 0.767799\n",
      "Epoch 4, Batch 51 loss: 0.824841 MAE 1.064682\n",
      "Epoch 4, Batch 101 loss: 0.778640 MAE 0.781288\n",
      "Epoch 4, Batch 151 loss: 0.790376 MAE 0.598589\n",
      "Epoch 4, Batch 201 loss: 0.754836 MAE 1.202478\n",
      "Epoch 4, Batch 251 loss: 0.724690 MAE 0.540731\n",
      "Epoch 4, Batch 301 loss: 0.721251 MAE 0.353294\n",
      "Epoch: 4 \tTraining Loss: 0.729014 \tValidation Loss: 1.598101 \t MAE 1.598027\n"
     ]
    }
   ],
   "source": [
    "model_reg = train_model_reg(model_reg, criterion_reg, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = torch.load(OUTPUT_PATH / 'model_classification.pt')\n",
    "model_cl.eval();\n",
    "\n",
    "model_reg = torch.load(OUTPUT_PATH / 'model_regression.pt')\n",
    "model_reg.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGetter(Dataset):\n",
    "    def __init__(self, image_paths, train=True, transform=None):\n",
    " \n",
    "        self.image_paths = image_paths \n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=cv2.imread(self.image_paths[idx])[225:375, 225:375, :]\n",
    "        \n",
    "        sample={\n",
    "            'image': np.uint8(image)\n",
    "            }\n",
    "\n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample['image']=self.transform(sample['image'])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test = glob.glob(str(PRIVATE_PATH / '**/*.png'), recursive=True)\n",
    "public_test = glob.glob(str(PUBLIC_PATH / '**/*.png'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_getter = TestDataGetter(private_test, transform=augs)\n",
    "public_test_getter = TestDataGetter(public_test, transform=augs)\n",
    "\n",
    "private_test_dataloader = DataLoader(private_test_getter, batch_size=32, shuffle=False) #, num_workers=2\n",
    "public_test_dataloader = DataLoader(public_test_getter, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    754\n",
       "0    748\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# public set\n",
    "\n",
    "public_predictions = []\n",
    "for batch in public_test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds = model_cl(imgs)\n",
    "    preds = preds['particle_class'].cpu().detach().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    public_predictions.extend(preds)\n",
    "pd.Series(public_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public set\n",
    "\n",
    "public_predictions = []\n",
    "for batch in public_test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds = model_reg(imgs)\n",
    "    preds = preds['particle_energy'].cpu().detach().numpy().reshape(-1)\n",
    "    \n",
    "    public_predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(public_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-c3a228adb8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprivate_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprivate_test_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-ff44b226d7af>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         sample={\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # private set\n",
    "\n",
    "# private_predictions = []\n",
    "# for batch in private_test_dataloader:\n",
    "#     imgs = batch['image'].to(device)\n",
    "#     preds = model(imgs)\n",
    "#     preds = preds['particle_class'].cpu().detach().numpy()\n",
    "#     preds = np.argmax(preds, axis = 1)\n",
    "    \n",
    "#     private_predictions.extend(preds)\n",
    "# pd.Series(private_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = torch.load(OUTPUT_PATH / 'model_classification.pt')\n",
    "model_cl.eval();\n",
    "\n",
    "model_reg = torch.load(OUTPUT_PATH / 'model_regression.pt')\n",
    "model_reg.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(INPUT_PATH / 'track1_predictions_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_ids = [t.split('/')[-1].split('.')[0] for t in private_test]\n",
    "public_ids = [t.split('/')[-1].split('.')[0] for t in public_test]\n",
    "\n",
    "sample_submission['path'] = sample_submission['id'].apply(lambda x: PRIVATE_PATH / f'{x}.png' if x in private_ids else PUBLIC_PATH / f'{x}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_getter = TestDataGetter(sample_submission['path'].apply(str).values, transform=augs)\n",
    "test_dataloader = DataLoader(test_getter, batch_size=32, shuffle=False) #, num_workers=2\n",
    "\n",
    "\n",
    "predictions_class = []\n",
    "predictions_energy = []\n",
    "for batch in test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds_class = model_cl(imgs)\n",
    "    preds_class = preds_class['particle_class'].cpu().detach().numpy()\n",
    "    preds_class = preds_class[:, 1]\n",
    "    \n",
    "    preds_energy = model_reg(imgs)\n",
    "    preds_energy = preds_energy['particle_energy'].cpu().detach().numpy().reshape(-1)\n",
    "    \n",
    "    predictions_class.extend(preds_class)\n",
    "    predictions_energy.extend(preds_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['classification_predictions'] = predictions_class\n",
    "sample_submission['regression_predictions'] = predictions_energy\n",
    "\n",
    "sample_submission.drop(columns = ['path']).to_csv(OUTPUT_PATH / 'predictions.csv', index  = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>classification_predictions</th>\n",
       "      <th>regression_predictions</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9a8b8bfc7a06afd346ff1c88f1f7a03316a9bc76</td>\n",
       "      <td>4.378097e-11</td>\n",
       "      <td>2.684487</td>\n",
       "      <td>../input/idao_dataset/public_test/9a8b8bfc7a06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2caa5748f814bbdbc64f4db43d7412ce359a777b</td>\n",
       "      <td>7.397206e-15</td>\n",
       "      <td>9.933042</td>\n",
       "      <td>../input/idao_dataset/public_test/2caa5748f814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b879970a23fc2a3b093bce85096808f13eaa69fb</td>\n",
       "      <td>1.129587e-05</td>\n",
       "      <td>9.469651</td>\n",
       "      <td>../input/idao_dataset/public_test/b879970a23fc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4d3cb5abbdc11518bb67ae7f2c415de808effb3</td>\n",
       "      <td>2.002822e-08</td>\n",
       "      <td>10.644419</td>\n",
       "      <td>../input/idao_dataset/public_test/a4d3cb5abbdc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>df8de9207196305057f73cea03d265ba720cb6e1</td>\n",
       "      <td>7.247805e-09</td>\n",
       "      <td>31.904327</td>\n",
       "      <td>../input/idao_dataset/public_test/df8de9207196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0908d699b9771b6c79d4c09ac6c3b9ce8f9372d8</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>21.194674</td>\n",
       "      <td>../input/idao_dataset/public_test/0908d699b977...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>b29ed14cc577cb33d2e09028b7e29cbe516d8af7</td>\n",
       "      <td>9.999914e-01</td>\n",
       "      <td>0.856617</td>\n",
       "      <td>../input/idao_dataset/public_test/b29ed14cc577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>eec069e158396764b065534e21d9ec7c71d2bc2a</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>20.134024</td>\n",
       "      <td>../input/idao_dataset/public_test/eec069e15839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>135b71b5f9c9d3504b8a580afee0e9ea09f0aa6e</td>\n",
       "      <td>9.999989e-01</td>\n",
       "      <td>0.851295</td>\n",
       "      <td>../input/idao_dataset/public_test/135b71b5f9c9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0851c743440bb0bafa222e4bbedfaf2f66ac7209</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>5.565408</td>\n",
       "      <td>../input/idao_dataset/public_test/0851c743440b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  classification_predictions  \\\n",
       "0     9a8b8bfc7a06afd346ff1c88f1f7a03316a9bc76                4.378097e-11   \n",
       "1     2caa5748f814bbdbc64f4db43d7412ce359a777b                7.397206e-15   \n",
       "2     b879970a23fc2a3b093bce85096808f13eaa69fb                1.129587e-05   \n",
       "3     a4d3cb5abbdc11518bb67ae7f2c415de808effb3                2.002822e-08   \n",
       "4     df8de9207196305057f73cea03d265ba720cb6e1                7.247805e-09   \n",
       "...                                        ...                         ...   \n",
       "1495  0908d699b9771b6c79d4c09ac6c3b9ce8f9372d8                1.000000e+00   \n",
       "1496  b29ed14cc577cb33d2e09028b7e29cbe516d8af7                9.999914e-01   \n",
       "1497  eec069e158396764b065534e21d9ec7c71d2bc2a                1.000000e+00   \n",
       "1498  135b71b5f9c9d3504b8a580afee0e9ea09f0aa6e                9.999989e-01   \n",
       "1499  0851c743440bb0bafa222e4bbedfaf2f66ac7209                9.999998e-01   \n",
       "\n",
       "      regression_predictions  \\\n",
       "0                   2.684487   \n",
       "1                   9.933042   \n",
       "2                   9.469651   \n",
       "3                  10.644419   \n",
       "4                  31.904327   \n",
       "...                      ...   \n",
       "1495               21.194674   \n",
       "1496                0.856617   \n",
       "1497               20.134024   \n",
       "1498                0.851295   \n",
       "1499                5.565408   \n",
       "\n",
       "                                                   path  \n",
       "0     ../input/idao_dataset/public_test/9a8b8bfc7a06...  \n",
       "1     ../input/idao_dataset/public_test/2caa5748f814...  \n",
       "2     ../input/idao_dataset/public_test/b879970a23fc...  \n",
       "3     ../input/idao_dataset/public_test/a4d3cb5abbdc...  \n",
       "4     ../input/idao_dataset/public_test/df8de9207196...  \n",
       "...                                                 ...  \n",
       "1495  ../input/idao_dataset/public_test/0908d699b977...  \n",
       "1496  ../input/idao_dataset/public_test/b29ed14cc577...  \n",
       "1497  ../input/idao_dataset/public_test/eec069e15839...  \n",
       "1498  ../input/idao_dataset/public_test/135b71b5f9c9...  \n",
       "1499  ../input/idao_dataset/public_test/0851c743440b...  \n",
       "\n",
       "[1500 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLIC_SET_BOUNDARY = 1502\n",
    "prediction_indexes = {\n",
    "    'public_test_er': [0, 750],\n",
    "    'public_test_he': [750, PUBLIC_SET_BOUNDARY],\n",
    "    'private_test_er': '?',\n",
    "    'private_test_he': '?'\n",
    "}\n",
    "\n",
    "prediction_allowed_values = {\n",
    "    'public_test_er': [3, 10, 30],\n",
    "    'public_test_he': [1, 6, 20],\n",
    "    'private_test_er':  [1, 6, 20],\n",
    "    'private_test_he': [3, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctPredictions(x, allowedPreds = np.array([1,2,3])):\n",
    "    preds = x.copy()\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i] = allowedPreds[np.argmin(np.abs(preds[i] - allowedPreds))]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "part = 'public_test_er'\n",
    "values = sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2].reset_index(drop = True)\n",
    "sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2] = correctPredictions(values, prediction_allowed_values[part]).values\n",
    "\n",
    "part = 'public_test_he'\n",
    "values = sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2].reset_index(drop = True)\n",
    "sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2] = correctPredictions(values, prediction_allowed_values[part]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.drop(columns = ['path']).to_csv(OUTPUT_PATH / 'predictions_postprocessed.csv', index  = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
