{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models, utils\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as F\n",
    "from skimage import io, transform\n",
    "from torch.optim import lr_scheduler\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "INPUT_PATH = Path('../input')\n",
    "OUTPUT_PATH = Path('../output')\n",
    "TRAIN_PATH = INPUT_PATH / 'idao_dataset' / 'train'\n",
    "PRIVATE_PATH = INPUT_PATH / 'idao_dataset' / 'private_test'\n",
    "PUBLIC_PATH = INPUT_PATH / 'idao_dataset' / 'public_test'\n",
    "\n",
    "RANDOM_SEED = 4444\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper1 = {\n",
    "    1:[1, 0, 0, 0, 0, 0],\n",
    "    3:[0, 1, 0, 0, 0, 0],\n",
    "    6:[0, 0, 1, 0, 0, 0],\n",
    "    10:[0, 0, 0, 1, 0, 0],\n",
    "    20:[0, 0, 0, 0, 1, 0],\n",
    "    30:[0, 0, 0, 0, 0, 1],\n",
    "}\n",
    "\n",
    "mapper2 = {\n",
    "    1:0,\n",
    "    3:1,\n",
    "    6:2,\n",
    "    10:3,\n",
    "    20:4,\n",
    "    30:5,\n",
    "}\n",
    "\n",
    "reverse_mapping = {\n",
    "    0: 1,\n",
    "    1: 3,\n",
    "    2: 6,\n",
    "    3: 10,\n",
    "    4: 20,\n",
    "    5: 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = glob.glob(str(TRAIN_PATH / '**/*.png'), recursive=True)\n",
    "\n",
    "train_images, test_images = train_test_split(images, shuffle = True, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metric(y_binary_true, y_binary_pred, y_reg_true, y_reg_pred):\n",
    "    '''\n",
    "    Competition metric\n",
    "    '''\n",
    "    \n",
    "    roc = roc_auc_score(y_binary_true, y_binary_pred)\n",
    "    mae = mean_absolute_error(y_reg_true, y_reg_pred)\n",
    "    return 1000 * (roc - mae), roc, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGetter(Dataset):\n",
    "    def __init__(self, image_paths, train=True, transform=None):\n",
    " \n",
    "        self.image_paths = image_paths \n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=cv2.imread(self.image_paths[idx])\n",
    "        if len(self.image_paths[idx].split('_')) == 18:\n",
    "            particle_class = 0 # ER\n",
    "            particle_energy = int(self.image_paths[idx].split('_')[7])\n",
    "        else:\n",
    "            particle_class = 1 # HE\n",
    "            particle_energy = int(self.image_paths[idx].split('_')[8])\n",
    "        \n",
    "        sample={\n",
    "            'image': np.uint8(image), \n",
    "            'particle_class': particle_class,\n",
    "            'particle_energy': particle_energy\n",
    "            }\n",
    "\n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample['image']=self.transform(sample['image'])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([128,128]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_data = DataGetter(train_images, train=True, transform=augs)\n",
    "transformed_test_data = DataGetter(test_images, train=False, transform=augs)\n",
    "\n",
    "train_dataloader = DataLoader(transformed_train_data, batch_size=32, shuffle=True) #, num_workers=2\n",
    "test_dataloader = DataLoader(transformed_test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CNNClassification(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(CNNClassification, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.model = models.resnet18()\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.fc0 = nn.Linear(512, 64)\n",
    "        self.fc1 = nn.Linear(64, 2)  # For classification\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.relu(self.fc0(x))\n",
    "        particle_class = torch.softmax(self.fc1(x), dim = 1)\n",
    "        return {'particle_class': particle_class}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_cl = CNNClassification(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in train_dataloader:\n",
    "#     img = item['image']\n",
    "#     break\n",
    "# model_CNN(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For binary output:particle_class\n",
    "criterion_binary= nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cl.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion_binary, optimizer, n_epochs=5):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(1, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        comp_metric = 0 \n",
    "        comp_val_metric = 0\n",
    "        batches = 0\n",
    "        val_batches = 0\n",
    "        max_batches = 256\n",
    "        # train the model #\n",
    "        model.train()\n",
    "        for batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "            if batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_class = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_class'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_class = output['particle_class']\n",
    "    \n",
    "            \n",
    "            particle_class = particle_class.squeeze().type(torch.LongTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_binary = label_class.cpu().detach().numpy()[:, 1]\n",
    "            y_true_binary = particle_class.cpu().detach().numpy()\n",
    "            \n",
    "            roc = roc_auc_score(y_true_binary, y_pred_binary) \n",
    "\n",
    "            loss_binary = criterion_binary(label_class, particle_class)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_binary\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f ROC AUC %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss, roc))\n",
    "            batches += 1\n",
    "        # validate the model #\n",
    "        model.eval()\n",
    "        \n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "            if val_batches > max_batches:\n",
    "                break\n",
    "            image, particle_class = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_class'].to(device)\n",
    "                                              \n",
    "            \n",
    "            output = model(image)\n",
    "\n",
    "            label_class = output['particle_class']\n",
    "\n",
    "            \n",
    "            particle_class = particle_class.squeeze().type(torch.LongTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_binary = label_class.cpu().detach().numpy()[:, 1]\n",
    "            y_true_binary = particle_class.cpu().detach().numpy()\n",
    "            \n",
    "            y_preds.extend(list(y_pred_binary))\n",
    "            y_trues.extend(list(y_true_binary))\n",
    "            \n",
    "            # calculate loss\n",
    "            loss_binary = criterion_binary(label_class, particle_class)\n",
    "            \n",
    "            loss = loss_binary\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            val_batches += 1\n",
    "        \n",
    "        roc_val = roc_auc_score(y_trues, y_preds) \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t ROC AUC {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss, roc_val))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model, OUTPUT_PATH / 'model_classification.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1 loss: 0.694436 ROC AUC 0.517647\n",
      "Epoch 1, Batch 51 loss: 0.419119 ROC AUC 1.000000\n",
      "Epoch 1, Batch 101 loss: 0.383787 ROC AUC 1.000000\n",
      "Epoch 1, Batch 151 loss: 0.365572 ROC AUC 1.000000\n",
      "Epoch 1, Batch 201 loss: 0.353748 ROC AUC 1.000000\n",
      "Epoch 1, Batch 251 loss: 0.348886 ROC AUC 1.000000\n",
      "Epoch: 1 \tTraining Loss: 0.348269 \tValidation Loss: 0.814128 \t ROC AUC 0.985322\n",
      "Validation loss decreased (inf --> 0.814128).  Saving model ...\n",
      "Epoch 2, Batch 1 loss: 0.313323 ROC AUC 1.000000\n",
      "Epoch 2, Batch 51 loss: 0.320365 ROC AUC 1.000000\n",
      "Epoch 2, Batch 101 loss: 0.320129 ROC AUC 1.000000\n",
      "Epoch 2, Batch 151 loss: 0.327909 ROC AUC 1.000000\n",
      "Epoch 2, Batch 201 loss: 0.326355 ROC AUC 1.000000\n",
      "Epoch 2, Batch 251 loss: 0.324331 ROC AUC 1.000000\n",
      "Epoch: 2 \tTraining Loss: 0.324073 \tValidation Loss: 0.314672 \t ROC AUC 0.999992\n",
      "Validation loss decreased (0.814128 --> 0.314672).  Saving model ...\n",
      "Epoch 3, Batch 1 loss: 0.313262 ROC AUC 1.000000\n",
      "Epoch 3, Batch 51 loss: 0.364007 ROC AUC 0.874494\n",
      "Epoch 3, Batch 101 loss: 0.367166 ROC AUC 1.000000\n",
      "Epoch 3, Batch 151 loss: 0.352705 ROC AUC 1.000000\n",
      "Epoch 3, Batch 201 loss: 0.343385 ROC AUC 1.000000\n",
      "Epoch 3, Batch 251 loss: 0.337898 ROC AUC 1.000000\n",
      "Epoch: 3 \tTraining Loss: 0.338648 \tValidation Loss: 0.814245 \t ROC AUC 0.500000\n",
      "Epoch 4, Batch 1 loss: 0.427270 ROC AUC 0.956250\n",
      "Epoch 4, Batch 51 loss: 0.343139 ROC AUC 0.996078\n",
      "Epoch 4, Batch 101 loss: 0.332456 ROC AUC 1.000000\n",
      "Epoch 4, Batch 151 loss: 0.327119 ROC AUC 1.000000\n",
      "Epoch 4, Batch 201 loss: 0.331093 ROC AUC 1.000000\n",
      "Epoch 4, Batch 251 loss: 0.329551 ROC AUC 1.000000\n",
      "Epoch: 4 \tTraining Loss: 0.329359 \tValidation Loss: 0.324080 \t ROC AUC 0.997304\n"
     ]
    }
   ],
   "source": [
    "model_cl = train_model(model_cl, criterion_binary, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class CNNReg(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(CNNReg, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "        else:\n",
    "            self.model = models.resnet18()\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.fc0 = nn.Linear(512, 64)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.relu(self.fc0(x))\n",
    "        particle_energy = self.fc1(x)\n",
    "        return {'particle_energy': particle_energy}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_reg = CNNReg(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_reg= nn.L1Loss()\n",
    "optimizer = optim.Adam(model_reg.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'particle_energy': tensor([[-0.0255],\n",
       "         [-0.0664],\n",
       "         [-0.0043],\n",
       "         [-0.0309],\n",
       "         [-0.0711],\n",
       "         [-0.0333],\n",
       "         [-0.1106],\n",
       "         [-0.0597],\n",
       "         [-0.0341],\n",
       "         [-0.0332],\n",
       "         [ 0.0095],\n",
       "         [-0.0983],\n",
       "         [-0.1056],\n",
       "         [-0.0271],\n",
       "         [-0.0753],\n",
       "         [-0.0827],\n",
       "         [-0.0562],\n",
       "         [-0.0565],\n",
       "         [-0.0620],\n",
       "         [ 0.0111],\n",
       "         [-0.0771],\n",
       "         [-0.0611],\n",
       "         [-0.0417],\n",
       "         [-0.0589],\n",
       "         [-0.0124],\n",
       "         [-0.0313],\n",
       "         [-0.1279],\n",
       "         [-0.0512],\n",
       "         [-0.0298],\n",
       "         [-0.0297],\n",
       "         [-0.0540],\n",
       "         [-0.0438]], device='cuda:0', grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in train_dataloader:\n",
    "    img = item['image']\n",
    "    break\n",
    "model_reg(img.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_reg(model, criterion_reg, optimizer, n_epochs=5):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    for epoch in range(1, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        comp_metric = 0 \n",
    "        comp_val_metric = 0\n",
    "        batches = 0\n",
    "        val_batches = 0\n",
    "        max_batches = 256\n",
    "        # train the model #\n",
    "        model.train()\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(train_dataloader):\n",
    "            if batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_energy = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_energy'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_energy = output['particle_energy']\n",
    "    \n",
    "            \n",
    "            particle_energy = particle_energy.squeeze().type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_reg = label_energy.cpu().detach().numpy()\n",
    "            #y_pred_reg = [reverse_mapping[x] for x in list(np.argmax(y_pred_reg, axis = 1))]\n",
    "            \n",
    "            y_true_reg = particle_energy.cpu().detach().numpy()\n",
    "            #y_true_reg = [reverse_mapping[x] for x in list(y_true_reg)]\n",
    "            \n",
    "            \n",
    "            mae = mean_absolute_error(y_true_reg, y_pred_reg) \n",
    "\n",
    "            loss_reg = criterion_reg(label_energy.reshape(-1), particle_energy)\n",
    "            \n",
    "            y_preds.extend(list(y_pred_reg))\n",
    "            y_trues.extend(list(y_true_reg))\n",
    "            \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            loss = loss_reg\n",
    "            # back prop\n",
    "            loss.backward()\n",
    "            # grad\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch %d, Batch %d loss: %.6f MAE %.6f' %\n",
    "                  (epoch, batch_idx + 1, train_loss, mae))\n",
    "            batches += 1\n",
    "        # validate the model #\n",
    "        model.eval()\n",
    "        \n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        for batch_idx, sample_batched in enumerate(test_dataloader):\n",
    "            if val_batches > max_batches:\n",
    "                break\n",
    "            # importing data and moving to GPU\n",
    "            image, particle_energy = sample_batched['image'].to(device),\\\n",
    "                                             sample_batched['particle_energy'].to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            label_energy = output['particle_energy']\n",
    "\n",
    "            \n",
    "            particle_energy = particle_energy.squeeze().type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            \n",
    "            y_pred_reg = label_energy.cpu().detach().numpy()\n",
    "            #y_pred_reg = [reverse_mapping[x] for x in list(np.argmax(y_pred_reg, axis = 1))]\n",
    "            \n",
    "            y_true_reg = particle_energy.cpu().detach().numpy()\n",
    "            #y_true_reg = [reverse_mapping[x] for x in list(y_true_reg)]\n",
    "            \n",
    "            y_preds.extend(list(y_pred_reg))\n",
    "            y_trues.extend(list(y_true_reg))\n",
    "            \n",
    "\n",
    "            loss_reg = criterion_reg(label_energy.reshape(-1), particle_energy)\n",
    "            \n",
    "            loss = loss_reg\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            val_batches += 1\n",
    "        \n",
    "        mae_val = mean_absolute_error(y_trues, y_preds) \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\t MAE {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss, mae_val))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            torch.save(model, OUTPUT_PATH / 'model_regression.pt')\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1 loss: 11.881562 MAE 11.881563\n",
      "Epoch 1, Batch 51 loss: 3.785183 MAE 2.001868\n",
      "Epoch 1, Batch 101 loss: 2.845062 MAE 1.975827\n",
      "Epoch 1, Batch 151 loss: 2.477610 MAE 2.870064\n",
      "Epoch 1, Batch 201 loss: 2.270208 MAE 0.798639\n",
      "Epoch 1, Batch 251 loss: 2.126450 MAE 2.523784\n",
      "Epoch: 1 \tTraining Loss: 2.107121 \tValidation Loss: 1.767886 \t MAE 1.767579\n",
      "Validation loss decreased (inf --> 1.767886).  Saving model ...\n",
      "Epoch 2, Batch 1 loss: 1.127111 MAE 1.127111\n",
      "Epoch 2, Batch 51 loss: 1.440896 MAE 1.172764\n",
      "Epoch 2, Batch 101 loss: 1.409017 MAE 1.086116\n",
      "Epoch 2, Batch 151 loss: 1.404670 MAE 0.848296\n",
      "Epoch 2, Batch 201 loss: 1.353096 MAE 0.720378\n",
      "Epoch 2, Batch 251 loss: 1.304661 MAE 2.185793\n",
      "Epoch: 2 \tTraining Loss: 1.302291 \tValidation Loss: 2.223934 \t MAE 2.225653\n",
      "Epoch 3, Batch 1 loss: 1.096357 MAE 1.096357\n",
      "Epoch 3, Batch 51 loss: 0.961057 MAE 0.516300\n",
      "Epoch 3, Batch 101 loss: 1.091048 MAE 2.216814\n",
      "Epoch 3, Batch 151 loss: 1.018790 MAE 0.817638\n",
      "Epoch 3, Batch 201 loss: 0.976668 MAE 0.937109\n",
      "Epoch 3, Batch 251 loss: 1.005086 MAE 0.903972\n",
      "Epoch: 3 \tTraining Loss: 1.003465 \tValidation Loss: 0.831816 \t MAE 0.831134\n",
      "Validation loss decreased (1.767886 --> 0.831816).  Saving model ...\n",
      "Epoch 4, Batch 1 loss: 0.855919 MAE 0.855919\n",
      "Epoch 4, Batch 51 loss: 0.745088 MAE 0.425448\n",
      "Epoch 4, Batch 101 loss: 0.684411 MAE 0.485716\n",
      "Epoch 4, Batch 151 loss: 0.665770 MAE 0.500756\n",
      "Epoch 4, Batch 201 loss: 0.627487 MAE 0.547383\n",
      "Epoch 4, Batch 251 loss: 0.589130 MAE 0.317363\n",
      "Epoch: 4 \tTraining Loss: 0.583383 \tValidation Loss: 0.346250 \t MAE 0.345867\n",
      "Validation loss decreased (0.831816 --> 0.346250).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model_reg = train_model_reg(model_reg, criterion_reg, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = torch.load(OUTPUT_PATH / 'model_classification.pt')\n",
    "model_cl.eval();\n",
    "\n",
    "model_reg = torch.load(OUTPUT_PATH / 'model_regression.pt')\n",
    "model_reg.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGetter(Dataset):\n",
    "    def __init__(self, image_paths, train=True, transform=None):\n",
    " \n",
    "        self.image_paths = image_paths \n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image=cv2.imread(self.image_paths[idx])\n",
    "        \n",
    "        sample={\n",
    "            'image': np.uint8(image)\n",
    "            }\n",
    "\n",
    "        #Applying transformation\n",
    "        if self.transform:\n",
    "            sample['image']=self.transform(sample['image'])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test = glob.glob(str(PRIVATE_PATH / '**/*.png'), recursive=True)\n",
    "public_test = glob.glob(str(PUBLIC_PATH / '**/*.png'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_getter = TestDataGetter(private_test, transform=augs)\n",
    "public_test_getter = TestDataGetter(public_test, transform=augs)\n",
    "\n",
    "private_test_dataloader = DataLoader(private_test_getter, batch_size=32, shuffle=False) #, num_workers=2\n",
    "public_test_dataloader = DataLoader(public_test_getter, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    755\n",
       "0    747\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# public set\n",
    "\n",
    "public_predictions = []\n",
    "for batch in public_test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds = model_cl(imgs)\n",
    "    preds = preds['particle_class'].cpu().detach().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    public_predictions.extend(preds)\n",
    "pd.Series(public_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public set\n",
    "\n",
    "public_predictions = []\n",
    "for batch in public_test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds = model_reg(imgs)\n",
    "    preds = preds['particle_energy'].cpu().detach().numpy().reshape(-1)\n",
    "    \n",
    "    public_predictions.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(public_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-c3a228adb8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprivate_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprivate_test_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-ff44b226d7af>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         sample={\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# private set\n",
    "\n",
    "private_predictions = []\n",
    "for batch in private_test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds = model(imgs)\n",
    "    preds = preds['particle_class'].cpu().detach().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    private_predictions.extend(preds)\n",
    "pd.Series(private_predictions).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = torch.load(OUTPUT_PATH / 'model_classification.pt')\n",
    "model_cl.eval();\n",
    "\n",
    "model_reg = torch.load(OUTPUT_PATH / 'model_regression.pt')\n",
    "model_reg.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(INPUT_PATH / 'track1_predictions_example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_ids = [t.split('/')[-1].split('.')[0] for t in private_test]\n",
    "public_ids = [t.split('/')[-1].split('.')[0] for t in public_test]\n",
    "\n",
    "sample_submission['path'] = sample_submission['id'].apply(lambda x: PRIVATE_PATH / f'{x}.png' if x in private_ids else PUBLIC_PATH / f'{x}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_getter = TestDataGetter(sample_submission['path'].apply(str).values, transform=augs)\n",
    "test_dataloader = DataLoader(test_getter, batch_size=32, shuffle=False) #, num_workers=2\n",
    "\n",
    "\n",
    "predictions_class = []\n",
    "predictions_energy = []\n",
    "for batch in test_dataloader:\n",
    "    imgs = batch['image'].to(device)\n",
    "    preds_class = model_cl(imgs)\n",
    "    preds_class = preds_class['particle_class'].cpu().detach().numpy()\n",
    "    preds_class = np.argmax(preds_class, axis = 1)\n",
    "    \n",
    "    preds_energy = model_reg(imgs)\n",
    "    preds_energy = preds_energy['particle_energy'].cpu().detach().numpy().reshape(-1)\n",
    "    \n",
    "    predictions_class.extend(preds_class)\n",
    "    predictions_energy.extend(preds_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['classification_predictions'] = predictions_class\n",
    "sample_submission['regression_predictions'] = predictions_energy\n",
    "\n",
    "sample_submission.drop(columns = ['path']).to_csv(OUTPUT_PATH / 'predictions.csv', index  = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLIC_SET_BOUNDARY = 1502\n",
    "prediction_indexes = {\n",
    "    'public_test_er': [0, 750],\n",
    "    'public_test_he': [750, PUBLIC_SET_BOUNDARY],\n",
    "    'private_test_er': '?',\n",
    "    'private_test_he': '?'\n",
    "}\n",
    "\n",
    "prediction_allowed_values = {\n",
    "    'public_test_er': [3, 10, 30],\n",
    "    'public_test_he': [1, 6, 20],\n",
    "    'private_test_er':  [1, 6, 20],\n",
    "    'private_test_he': [3, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctPredictions(x, allowedPreds = np.array([1,2,3])):\n",
    "    preds = x.copy()\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i] = allowedPreds[np.argmin(np.abs(preds[i] - allowedPreds))]\n",
    "    \n",
    "    return preds\n",
    "\n",
    "part = 'public_test_er'\n",
    "values = sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2].reset_index(drop = True)\n",
    "sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2] = correctPredictions(values, prediction_allowed_values[part]).values\n",
    "\n",
    "part = 'public_test_he'\n",
    "values = sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2].reset_index(drop = True)\n",
    "sample_submission.iloc[prediction_indexes[part][0] : prediction_indexes[part][1], 2] = correctPredictions(values, prediction_allowed_values[part]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.drop(columns = ['path']).to_csv(OUTPUT_PATH / 'predictions_postprocessed.csv', index  = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7550\n",
       "0      12\n",
       "Name: classification_predictions, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[PUBLIC_SET_BOUNDARY+7500:]['classification_predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5362\n",
       "1    2135\n",
       "Name: classification_predictions, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[PUBLIC_SET_BOUNDARY:PUBLIC_SET_BOUNDARY + 7500-3]['classification_predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7555\n",
       "0      12\n",
       "Name: classification_predictions, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[1500 + 7500-3:]['classification_predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "private_test    15062\n",
       "Name: path, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission['path'].apply(lambda x: str(x).split('/')[-2]).iloc[1502:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowedPreds = np.array([1,2,3])\n",
    "allowedPreds[np.argmin(np.abs(10.8 - allowedPreds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>classification_predictions</th>\n",
       "      <th>regression_predictions</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>d88edebbf8247af207f19294fd7d1514fd220431</td>\n",
       "      <td>1</td>\n",
       "      <td>1.023144</td>\n",
       "      <td>../input/idao_dataset/public_test/d88edebbf824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>a4c526a9038cf3bfb8a0ad85cf0e7b03f277eb0a</td>\n",
       "      <td>1</td>\n",
       "      <td>1.068277</td>\n",
       "      <td>../input/idao_dataset/public_test/a4c526a9038c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0c4e389c867a634186b93e511a7d19d2591b668b</td>\n",
       "      <td>1</td>\n",
       "      <td>1.027634</td>\n",
       "      <td>../input/idao_dataset/public_test/0c4e389c867a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>c64d46234689a9616f3bb48389ca60eb0286f875</td>\n",
       "      <td>1</td>\n",
       "      <td>5.801307</td>\n",
       "      <td>../input/idao_dataset/public_test/c64d46234689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>d05e5557914fb378c851d4e64687a008b11d0b94</td>\n",
       "      <td>1</td>\n",
       "      <td>19.992479</td>\n",
       "      <td>../input/idao_dataset/public_test/d05e5557914f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0851c743440bb0bafa222e4bbedfaf2f66ac7209</td>\n",
       "      <td>1</td>\n",
       "      <td>5.794290</td>\n",
       "      <td>../input/idao_dataset/public_test/0851c743440b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>7b2edfb684bcfdcf34d546cd9a4563a65ef8e7d4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.753428</td>\n",
       "      <td>../input/idao_dataset/public_test/7b2edfb684bc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>7b0074823b3301443ddfafe179ed44a412364c32</td>\n",
       "      <td>1</td>\n",
       "      <td>19.779394</td>\n",
       "      <td>../input/idao_dataset/public_test/7b0074823b33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>8dbaee16550363fe6f4e7f32ed6c8419926354ee</td>\n",
       "      <td>0</td>\n",
       "      <td>23.464390</td>\n",
       "      <td>../input/idao_dataset/private_test/8dbaee16550...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>4ffab3dc6aa90e7173886162b2ea591c2fdc6331</td>\n",
       "      <td>0</td>\n",
       "      <td>23.409683</td>\n",
       "      <td>../input/idao_dataset/private_test/4ffab3dc6aa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>754 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  classification_predictions  \\\n",
       "750   d88edebbf8247af207f19294fd7d1514fd220431                           1   \n",
       "751   a4c526a9038cf3bfb8a0ad85cf0e7b03f277eb0a                           1   \n",
       "752   0c4e389c867a634186b93e511a7d19d2591b668b                           1   \n",
       "753   c64d46234689a9616f3bb48389ca60eb0286f875                           1   \n",
       "754   d05e5557914fb378c851d4e64687a008b11d0b94                           1   \n",
       "...                                        ...                         ...   \n",
       "1499  0851c743440bb0bafa222e4bbedfaf2f66ac7209                           1   \n",
       "1500  7b2edfb684bcfdcf34d546cd9a4563a65ef8e7d4                           1   \n",
       "1501  7b0074823b3301443ddfafe179ed44a412364c32                           1   \n",
       "1502  8dbaee16550363fe6f4e7f32ed6c8419926354ee                           0   \n",
       "1503  4ffab3dc6aa90e7173886162b2ea591c2fdc6331                           0   \n",
       "\n",
       "      regression_predictions  \\\n",
       "750                 1.023144   \n",
       "751                 1.068277   \n",
       "752                 1.027634   \n",
       "753                 5.801307   \n",
       "754                19.992479   \n",
       "...                      ...   \n",
       "1499                5.794290   \n",
       "1500                5.753428   \n",
       "1501               19.779394   \n",
       "1502               23.464390   \n",
       "1503               23.409683   \n",
       "\n",
       "                                                   path  \n",
       "750   ../input/idao_dataset/public_test/d88edebbf824...  \n",
       "751   ../input/idao_dataset/public_test/a4c526a9038c...  \n",
       "752   ../input/idao_dataset/public_test/0c4e389c867a...  \n",
       "753   ../input/idao_dataset/public_test/c64d46234689...  \n",
       "754   ../input/idao_dataset/public_test/d05e5557914f...  \n",
       "...                                                 ...  \n",
       "1499  ../input/idao_dataset/public_test/0851c743440b...  \n",
       "1500  ../input/idao_dataset/public_test/7b2edfb684bc...  \n",
       "1501  ../input/idao_dataset/public_test/7b0074823b33...  \n",
       "1502  ../input/idao_dataset/private_test/8dbaee16550...  \n",
       "1503  ../input/idao_dataset/private_test/4ffab3dc6aa...  \n",
       "\n",
       "[754 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[PUV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.0\n",
       "1      3.0\n",
       "2      3.0\n",
       "3      3.0\n",
       "4      3.0\n",
       "      ... \n",
       "745    3.0\n",
       "746    3.0\n",
       "747    3.0\n",
       "748    3.0\n",
       "749    3.0\n",
       "Name: regression_predictions, Length: 750, dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correctPredictions(x, allowedPreds = np.array([1,2,3])):\n",
    "    preds = x.copy()\n",
    "    for i in range(preds.shape[0]):\n",
    "        preds[i] = allowedPreds[np.argmin(np.abs(preds[i] - allowedPreds))]\n",
    "    \n",
    "    return preds\n",
    "correctPredictions(sample_submission.iloc[:750, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>classification_predictions</th>\n",
       "      <th>regression_predictions</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9a8b8bfc7a06afd346ff1c88f1f7a03316a9bc76</td>\n",
       "      <td>0</td>\n",
       "      <td>2.782609</td>\n",
       "      <td>../input/idao_dataset/public_test/9a8b8bfc7a06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2caa5748f814bbdbc64f4db43d7412ce359a777b</td>\n",
       "      <td>0</td>\n",
       "      <td>9.784853</td>\n",
       "      <td>../input/idao_dataset/public_test/2caa5748f814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b879970a23fc2a3b093bce85096808f13eaa69fb</td>\n",
       "      <td>0</td>\n",
       "      <td>9.664519</td>\n",
       "      <td>../input/idao_dataset/public_test/b879970a23fc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a4d3cb5abbdc11518bb67ae7f2c415de808effb3</td>\n",
       "      <td>0</td>\n",
       "      <td>9.832215</td>\n",
       "      <td>../input/idao_dataset/public_test/a4d3cb5abbdc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>df8de9207196305057f73cea03d265ba720cb6e1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.029110</td>\n",
       "      <td>../input/idao_dataset/public_test/df8de9207196...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>f8bdabef01fcd90a653a4fd72fd75d02db9f557f</td>\n",
       "      <td>0</td>\n",
       "      <td>9.612540</td>\n",
       "      <td>../input/idao_dataset/public_test/f8bdabef01fc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>c292dc18588b7981b4cf7feec25ac86207332a56</td>\n",
       "      <td>0</td>\n",
       "      <td>2.809313</td>\n",
       "      <td>../input/idao_dataset/public_test/c292dc18588b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2786388c5b2e0c3ea3e3417803b39c7919210fe4</td>\n",
       "      <td>0</td>\n",
       "      <td>30.932081</td>\n",
       "      <td>../input/idao_dataset/public_test/2786388c5b2e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>74e6412eb02ffd302abe71b5407f893415bb8a3a</td>\n",
       "      <td>0</td>\n",
       "      <td>9.593667</td>\n",
       "      <td>../input/idao_dataset/public_test/74e6412eb02f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2c5f62eb55d6b7ddfeff6c075e15858434e81846</td>\n",
       "      <td>0</td>\n",
       "      <td>2.805414</td>\n",
       "      <td>../input/idao_dataset/public_test/2c5f62eb55d6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id  classification_predictions  \\\n",
       "0    9a8b8bfc7a06afd346ff1c88f1f7a03316a9bc76                           0   \n",
       "1    2caa5748f814bbdbc64f4db43d7412ce359a777b                           0   \n",
       "2    b879970a23fc2a3b093bce85096808f13eaa69fb                           0   \n",
       "3    a4d3cb5abbdc11518bb67ae7f2c415de808effb3                           0   \n",
       "4    df8de9207196305057f73cea03d265ba720cb6e1                           0   \n",
       "..                                        ...                         ...   \n",
       "745  f8bdabef01fcd90a653a4fd72fd75d02db9f557f                           0   \n",
       "746  c292dc18588b7981b4cf7feec25ac86207332a56                           0   \n",
       "747  2786388c5b2e0c3ea3e3417803b39c7919210fe4                           0   \n",
       "748  74e6412eb02ffd302abe71b5407f893415bb8a3a                           0   \n",
       "749  2c5f62eb55d6b7ddfeff6c075e15858434e81846                           0   \n",
       "\n",
       "     regression_predictions                                               path  \n",
       "0                  2.782609  ../input/idao_dataset/public_test/9a8b8bfc7a06...  \n",
       "1                  9.784853  ../input/idao_dataset/public_test/2caa5748f814...  \n",
       "2                  9.664519  ../input/idao_dataset/public_test/b879970a23fc...  \n",
       "3                  9.832215  ../input/idao_dataset/public_test/a4d3cb5abbdc...  \n",
       "4                 30.029110  ../input/idao_dataset/public_test/df8de9207196...  \n",
       "..                      ...                                                ...  \n",
       "745                9.612540  ../input/idao_dataset/public_test/f8bdabef01fc...  \n",
       "746                2.809313  ../input/idao_dataset/public_test/c292dc18588b...  \n",
       "747               30.932081  ../input/idao_dataset/public_test/2786388c5b2e...  \n",
       "748                9.593667  ../input/idao_dataset/public_test/74e6412eb02f...  \n",
       "749                2.805414  ../input/idao_dataset/public_test/2c5f62eb55d6...  \n",
       "\n",
       "[750 rows x 4 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.iloc[:750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15058"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7491 + 7567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7484"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2499 + 2501 + 2484"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7424"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2463 + 2503 + 2458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14908"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7484 + 7424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16564, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16408"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14908 + 1500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
